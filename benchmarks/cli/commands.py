"""
CLI ã‚³ãƒãƒ³ãƒ‰å®Ÿè£…

å„CLIã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…ã‚’è²¬å‹™åˆ¥ã«åˆ†é›¢ã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""
import json
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any

from benchmarks.core.config import BenchmarkConfigManager
from benchmarks.core.runner import UnifiedBenchmarkRunner
from benchmarks.core.validator import BenchmarkValidator, BenchmarkResultAnalyzer
from benchmarks.benchmark_result_manager import BenchmarkResultManager
from benchmarks.core.types import BenchmarkResult


class CommandExecutor:
    """ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, args):
        self.args = args
        self.verbose = getattr(args, 'verbose', False)
    
    def load_config(self):
        """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        config_manager = BenchmarkConfigManager(self.args.config)
        config = config_manager.load_config()
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§è¨­å®šã‚’ä¸Šæ›¸ã
        if hasattr(self.args, 'output_dir') and self.args.output_dir:
            config.output_dir = self.args.output_dir
        
        if hasattr(self.args, 'parallel') and self.args.parallel:
            config.parallel = True
        
        if hasattr(self.args, 'workers') and self.args.workers:
            config.max_workers = self.args.workers
        
        if hasattr(self.args, 'warmup') and self.args.warmup:
            config.warmup_runs = self.args.warmup
        
        if hasattr(self.args, 'runs') and self.args.runs:
            config.measurement_runs = self.args.runs
        
        if hasattr(self.args, 'timeout') and self.args.timeout:
            config.timeout_seconds = self.args.timeout
        
        if hasattr(self.args, 'no_charts') and self.args.no_charts:
            config.generate_charts = False
        
        # è¨­å®šã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        config_manager.validate_config(config)
        
        return config
    
    def print_verbose(self, message: str):
        """è©³ç´°å‡ºåŠ›"""
        if self.verbose:
            print(message)


class RunCommand(CommandExecutor):
    """runã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """runã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        config = self.load_config()
        self.print_verbose(f"è¨­å®š: {config}")
        
        # ãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆ
        runner = UnifiedBenchmarkRunner(config)
        
        try:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            results = self._run_benchmarks(runner)
            
            if not results:
                print("No benchmarks were executed")
                return 1
            
            # çµæœã®ä¿å­˜
            self._save_results(results, config)
            
            # çµæœã®åˆ†æã¨è¡¨ç¤º
            self._analyze_and_display_results(results)
            
            return 0
            
        except Exception as e:
            print(f"Error running benchmarks: {e}", file=sys.stderr)
            return 1
    
    def _run_benchmarks(self, runner: UnifiedBenchmarkRunner) -> List[BenchmarkResult]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        if hasattr(self.args, 'target') and self.args.target:
            results_dict = runner.run_specific_targets(self.args.target)
            return list(results_dict.values())
        else:
            all_results_dict = runner.run_all_benchmarks()
            # å…¨çµæœã‚’å¹³å¦ãªãƒªã‚¹ãƒˆã«å¤‰æ›
            all_results = []
            for plugin_results in all_results_dict.values():
                if isinstance(plugin_results, dict):
                    all_results.extend(plugin_results.values())
                elif isinstance(plugin_results, list):
                    all_results.extend(plugin_results)
                else:
                    # å˜ä¸€ã®BenchmarkResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    all_results.append(plugin_results)
            return all_results
    
    def _save_results(self, results: List[BenchmarkResult], config):
        """çµæœã‚’ä¿å­˜"""
        if not getattr(self.args, 'no_save', False):
            # ãƒªã‚¹ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            results_dict = {result.target_name: result for result in results}
            result_manager = BenchmarkResultManager(str(config.output_dir))
            saved_file = result_manager.save_results(results_dict)
            print(f"\nResults saved to: {saved_file}")
    
    def _analyze_and_display_results(self, results: List[BenchmarkResult]):
        """çµæœã‚’åˆ†æã—ã¦è¡¨ç¤º"""
        try:
            analyzer = BenchmarkResultAnalyzer()
            # ãƒªã‚¹ãƒˆã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            results_dict = {result.target_name: result for result in results}
            analysis = analyzer.analyze_results(results_dict)
            
            # è¦ç´„ã‚’è¡¨ç¤º
            self._display_summary(analysis["summary"])
            
            # æ¤œè¨¼çµæœã‚’è¡¨ç¤º
            validation = analysis["validation"]
            self._display_validation(validation)
        except Exception as e:
            print(f"Error in result analysis: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            raise
    
    def _display_summary(self, summary: Dict[str, Any]):
        """è¦ç´„ã‚’è¡¨ç¤º"""
        print(f"\n=== BENCHMARK SUMMARY ===")
        print(f"Total modules: {summary['total_modules']}")
        print(f"Successful: {summary['successful']}")
        print(f"Failed: {summary['failed']}")
        print(f"Success rate: {summary['success_rate']:.1%}")
        
        if summary['successful'] > 0:
            print(f"Fastest time: {summary['fastest_time']*1000:.3f}ms")
            print(f"Slowest time: {summary['slowest_time']*1000:.3f}ms")
            print(f"Average time: {summary['average_time']*1000:.3f}ms")
    
    def _display_validation(self, validation: Dict[str, Any]):
        """æ¤œè¨¼çµæœã‚’è¡¨ç¤º"""
        if validation["errors"]:
            print(f"\nâš ï¸  Validation errors: {len(validation['errors'])}")
            for error in validation["errors"][:5]:  # æœ€åˆã®5å€‹ã®ã¿è¡¨ç¤º
                print(f"  - {error}")
            if len(validation["errors"]) > 5:
                print(f"  ... and {len(validation['errors']) - 5} more")


class ListCommand(CommandExecutor):
    """listã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """listã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            config = self.load_config()
            runner = UnifiedBenchmarkRunner(config)
            
            targets = runner.list_available_targets()
            
            # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if hasattr(self.args, 'plugin') and self.args.plugin:
                targets = [t for t in targets if t.startswith(self.args.plugin)]
            
            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆåˆ¥å‡ºåŠ›
            format_type = getattr(self.args, 'format', 'table')
            self._display_targets(targets, format_type)
            
            return 0
            
        except Exception as e:
            print(f"Error listing targets: {e}", file=sys.stderr)
            return 1
    
    def _display_targets(self, targets: List[str], format_type: str):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æŒ‡å®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡¨ç¤º"""
        if format_type == "json":
            print(json.dumps(targets, indent=2))
        elif format_type == "yaml":
            print("targets:")
            for target in targets:
                print(f"  - {target}")
        else:  # table
            print("Available benchmark targets:")
            for target in targets:
                print(f"  {target}")


class ValidateCommand(CommandExecutor):
    """validateã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """validateã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            results_file = self.args.results_file
            
            if not results_file.exists():
                print(f"Results file not found: {results_file}", file=sys.stderr)
                return 1
            
            # çµæœã‚’èª­ã¿è¾¼ã¿
            with open(results_file, 'r') as f:
                results_data = json.load(f)
            
            # æ¤œè¨¼å®Ÿè¡Œ
            validator = BenchmarkValidator()
            validation_result = validator.validate_multiple_results(results_data)
            
            # ValidationResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã«å¤‰æ›
            validation_dict = {
                "is_valid": validation_result.is_valid,
                "errors": validation_result.errors,
                "warnings": validation_result.warnings,
                "metrics": validation_result.metrics
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            if hasattr(self.args, 'report') and self.args.report:
                self._generate_validation_report(validation_dict, self.args.report)
            else:
                self._display_validation_result(validation_dict)
            
            return 0 if validation_result.is_valid else 1
            
        except Exception as e:
            print(f"Error validating results: {e}", file=sys.stderr)
            return 1
    
    def _display_validation_result(self, result: Dict[str, Any]):
        """æ¤œè¨¼çµæœã‚’è¡¨ç¤º"""
        print(f"Validation result: {'PASS' if result['is_valid'] else 'FAIL'}")
        
        if result["errors"]:
            print(f"\nErrors ({len(result['errors'])}):")
            for error in result["errors"]:
                print(f"  - {error}")
        
        if result["warnings"]:
            print(f"\nWarnings ({len(result['warnings'])}):")
            for warning in result["warnings"]:
                print(f"  - {warning}")
    
    def _generate_validation_report(self, result: Dict[str, Any], report_path: Path):
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report_content = {
            "validation_status": "PASS" if result["is_valid"] else "FAIL",
            "timestamp": result.get("timestamp"),
            "errors": result["errors"],
            "warnings": result["warnings"],
            "statistics": result.get("statistics", {})
        }
        
        with open(report_path, 'w') as f:
            json.dump(report_content, f, indent=2)
        
        print(f"Validation report saved to: {report_path}")


class CompareCommand(CommandExecutor):
    """compareã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """compareã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            baseline_file = self.args.baseline
            current_file = self.args.current
            
            if not baseline_file.exists():
                print(f"Baseline file not found: {baseline_file}", file=sys.stderr)
                return 1
            
            if not current_file.exists():
                print(f"Current file not found: {current_file}", file=sys.stderr)
                return 1
            
            # çµæœã‚’èª­ã¿è¾¼ã¿
            with open(baseline_file, 'r') as f:
                baseline_data = json.load(f)
            
            with open(current_file, 'r') as f:
                current_data = json.load(f)
            
            # æ¯”è¼ƒå®Ÿè¡Œ
            analyzer = BenchmarkResultAnalyzer()
            comparison = analyzer.compare_results(baseline_data, current_data)
            
            # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
            self._display_comparison_result(comparison)
            
            # å›å¸°æ¤œå‡º
            threshold = getattr(self.args, 'regression_threshold', -0.1)
            regressions = [c for c in comparison if c.get('performance_change', 0) < threshold]
            
            if regressions:
                print(f"\nâš ï¸  Performance regressions detected: {len(regressions)}")
                return 1
            
            return 0
            
        except Exception as e:
            print(f"Error comparing results: {e}", file=sys.stderr)
            return 1
    
    def _display_comparison_result(self, comparison: List[Dict[str, Any]]):
        """æ¯”è¼ƒçµæœã‚’è¡¨ç¤º"""
        print("=== BENCHMARK COMPARISON ===")
        
        for result in comparison:
            target = result["target"]
            change = result.get("performance_change", 0)
            
            if change > 0.05:  # 5%ä»¥ä¸Šã®æ”¹å–„
                status = "ğŸš€ IMPROVED"
            elif change < -0.05:  # 5%ä»¥ä¸Šã®åŠ£åŒ–
                status = "ğŸŒ REGRESSED"
            else:
                status = "â¡ï¸  UNCHANGED"
            
            print(f"{target}: {status} ({change:+.1%})")


class ConfigCommand(CommandExecutor):
    """configã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè£…"""
    
    def execute(self) -> int:
        """configã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            action = getattr(self.args, 'config_action', None)
            
            if action == 'template':
                return self._create_template()
            elif action == 'show':
                return self._show_config()
            else:
                print("No config action specified", file=sys.stderr)
                return 1
                
        except Exception as e:
            print(f"Error in config command: {e}", file=sys.stderr)
            return 1
    
    def _create_template(self) -> int:
        """è¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ"""
        output_file = self.args.output_file
        config_manager = BenchmarkConfigManager()
        
        template = config_manager.generate_template()
        
        with open(output_file, 'w') as f:
            f.write(template)
        
        print(f"Configuration template created: {output_file}")
        return 0
    
    def _show_config(self) -> int:
        """ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º"""
        config = self.load_config()
        print("Current configuration:")
        print(json.dumps(config.__dict__, indent=2, default=str))
        return 0


# ã‚³ãƒãƒ³ãƒ‰ç™»éŒ²ãƒãƒƒãƒ—
COMMAND_MAP = {
    'run': RunCommand,
    'list': ListCommand,
    'validate': ValidateCommand,
    'compare': CompareCommand,
    'config': ConfigCommand,
}


def execute_command(command: str, args) -> int:
    """æŒ‡å®šã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
    if command not in COMMAND_MAP:
        print(f"Unknown command: {command}", file=sys.stderr)
        return 1
    
    command_class = COMMAND_MAP[command]
    executor = command_class(args)
    return executor.execute()