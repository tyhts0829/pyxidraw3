#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å›å¸°æ¤œå‡ºã®ãŸã‚ã®ãƒ†ã‚¹ãƒˆ
"""

import time
import unittest
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock
from statistics import mean, stdev

import numpy as np

from benchmarks.core.config import BenchmarkConfig
from benchmarks.core.runner import UnifiedBenchmarkRunner
from benchmarks.core.execution import BenchmarkExecutor
from benchmarks.benchmark_result_manager import BenchmarkResultManager
from benchmarks.plugins.base import PluginManager, BaseBenchmarkTarget
from benchmarks.core.types import BenchmarkResult, TimingData, BenchmarkMetrics
from benchmarks.core.exceptions import get_error_handler, get_error_collector
from engine.core.geometry import Geometry


class PerformanceBenchmarkTarget(BaseBenchmarkTarget):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ"""
    
    def __init__(self, name: str, work_complexity: int = 1000):
        self.work_complexity = work_complexity
        
        def performance_function():
            # CPUé›†ç´„çš„ãªä½œæ¥­ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            result = 0
            for i in range(self.work_complexity):
                result += i * i
            
            # Geometryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            vertices = np.random.rand(min(100, self.work_complexity // 10), 3).astype(np.float32)
            return Geometry.from_lines([vertices])
        
        super().__init__(name, performance_function)


class TestBenchmarkSystemPerformance(unittest.TestCase):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    
    @classmethod
    def setUpClass(cls):
        """ã‚¯ãƒ©ã‚¹å…¨ä½“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        cls.temp_dir = tempfile.mkdtemp()
        cls.config = BenchmarkConfig(
            warmup_runs=2,
            measurement_runs=5,
            parallel=False,
            output_dir=Path(cls.temp_dir)
        )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤ï¼ˆç§’ï¼‰
        cls.PERFORMANCE_THRESHOLDS = {
            'single_target_execution': 0.1,    # å˜ä¸€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®Ÿè¡Œæ™‚é–“
            'result_save_time': 0.05,          # çµæœä¿å­˜æ™‚é–“
            'result_load_time': 0.02,          # çµæœèª­ã¿è¾¼ã¿æ™‚é–“
            'plugin_discovery_time': 0.1,      # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç™ºè¦‹æ™‚é–“
            'benchmark_initialization': 0.05,  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–æ™‚é–“
        }
    
    @classmethod
    def tearDownClass(cls):
        """ã‚¯ãƒ©ã‚¹å…¨ä½“ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        import shutil
        shutil.rmtree(cls.temp_dir, ignore_errors=True)
    
    def setUp(self):
        """å„ãƒ†ã‚¹ãƒˆã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.performance_data = {}
    
    def measure_time(self, operation_name):
        """æ™‚é–“æ¸¬å®šç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        class TimeContext:
            def __init__(self, test_instance, name):
                self.test_instance = test_instance
                self.name = name
                self.start_time = None
            
            def __enter__(self):
                self.start_time = time.perf_counter()
                return self
            
            def __exit__(self, exc_type, exc_val, exc_tb):
                end_time = time.perf_counter()
                execution_time = end_time - self.start_time
                self.test_instance.performance_data[self.name] = execution_time
                
                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                threshold = self.test_instance.PERFORMANCE_THRESHOLDS.get(self.name)
                if threshold and execution_time > threshold:
                    print(f"âš ï¸  Performance warning: {self.name} took {execution_time:.3f}s (threshold: {threshold:.3f}s)")
        
        return TimeContext(self, operation_name)
    
    def test_single_target_execution_performance(self):
        """å˜ä¸€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®Ÿè¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        executor = BenchmarkExecutor(
            self.config,
            get_error_handler(),
            get_error_collector()
        )
        
        target = PerformanceBenchmarkTarget("perf_test", work_complexity=1000)
        
        with self.measure_time('single_target_execution'):
            result = executor.initialize_benchmark_result(target)
            executor.measure_target_characteristics(target, result)
            executor.execute_benchmark_measurements(target, result)
            executor.calculate_benchmark_statistics(result)
        
        # çµæœæ¤œè¨¼
        self.assertTrue(result.success)
        self.assertGreater(len(result.timing_data.measurement_times), 0)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
        execution_time = self.performance_data['single_target_execution']
        self.assertLess(execution_time, self.PERFORMANCE_THRESHOLDS['single_target_execution'],
                       f"Single target execution too slow: {execution_time:.3f}s")
    
    def test_result_persistence_performance(self):
        """çµæœæ°¸ç¶šåŒ–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # å¤§ããªçµæœãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
        results = {}
        for i in range(20):  # 20å€‹ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµæœ
            results[f"target_{i}"] = BenchmarkResult(
                target_name=f"target_{i}",
                plugin_name="perf_test_plugin",
                config={},
                timestamp=time.time(),
                success=True,
                error_message="",
                timing_data=TimingData(
                    warm_up_times=[0.001, 0.002],
                    measurement_times=[0.003, 0.004, 0.005, 0.003, 0.004],
                    total_time=0.019,
                    average_time=0.0038,
                    std_dev=0.0008,
                    min_time=0.003,
                    max_time=0.005
                ),
                metrics=BenchmarkMetrics(
                    vertices_count=100 * (i + 1),
                    geometry_complexity=1.5 * (i + 1),
                    memory_usage=1024 * (i + 1),
                    cache_hit_rate=0.8
                ),
                output_data={"data": list(range(i * 10))},  # å¯å¤‰ã‚µã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿
                serialization_overhead=0.0001
            )
        
        manager = BenchmarkResultManager(str(self.config.output_dir))
        
        # ä¿å­˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        with self.measure_time('result_save_time'):
            saved_file = manager.save_results(results)
        
        # èª­ã¿è¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        with self.measure_time('result_load_time'):
            loaded_results = manager.load_results(saved_file)
        
        # çµæœæ¤œè¨¼
        self.assertIsNotNone(loaded_results)
        self.assertEqual(len(loaded_results), 20)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
        save_time = self.performance_data['result_save_time']
        load_time = self.performance_data['result_load_time']
        
        self.assertLess(save_time, self.PERFORMANCE_THRESHOLDS['result_save_time'],
                       f"Result save too slow: {save_time:.3f}s")
        self.assertLess(load_time, self.PERFORMANCE_THRESHOLDS['result_load_time'],
                       f"Result load too slow: {load_time:.3f}s")
    
    def test_plugin_discovery_performance(self):
        """ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ç™ºè¦‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        with patch.object(PluginManager, '_auto_discover_plugins') as mock_discovery:
            # å®Ÿéš›ã®ç™ºè¦‹å‡¦ç†ã‚’ãƒ¢ãƒƒã‚¯
            mock_discovery.side_effect = lambda: time.sleep(0.01)  # è»½ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            with self.measure_time('plugin_discovery_time'):
                plugin_manager = PluginManager(self.config)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
            discovery_time = self.performance_data['plugin_discovery_time']
            self.assertLess(discovery_time, self.PERFORMANCE_THRESHOLDS['plugin_discovery_time'],
                           f"Plugin discovery too slow: {discovery_time:.3f}s")
    
    def test_benchmark_initialization_performance(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆæœŸåŒ–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        with patch.object(PluginManager, '_auto_discover_plugins'):
            with self.measure_time('benchmark_initialization'):
                runner = UnifiedBenchmarkRunner(self.config)
                
                # è¤‡æ•°ã®ãƒ¢ãƒƒã‚¯ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ç™»éŒ²
                for i in range(5):
                    mock_plugin = MagicMock()
                    mock_plugin.name = f"perf_plugin_{i}"
                    mock_plugin.get_targets.return_value = [
                        PerformanceBenchmarkTarget(f"target_{j}", 100) for j in range(3)
                    ]
                    runner.plugin_manager.register_plugin(mock_plugin)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
            init_time = self.performance_data['benchmark_initialization']
            self.assertLess(init_time, self.PERFORMANCE_THRESHOLDS['benchmark_initialization'],
                           f"Benchmark initialization too slow: {init_time:.3f}s")
    
    def test_parallel_vs_sequential_performance(self):
        """ä¸¦åˆ—å®Ÿè¡Œ vs é€æ¬¡å®Ÿè¡Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
        targets = [PerformanceBenchmarkTarget(f"parallel_target_{i}", 500) for i in range(4)]
        
        # é€æ¬¡å®Ÿè¡Œã®æ¸¬å®š
        sequential_config = BenchmarkConfig(
            warmup_runs=1,
            measurement_runs=3,
            parallel=False,
            output_dir=self.config.output_dir
        )
        
        with patch.object(PluginManager, '_auto_discover_plugins'):
            sequential_runner = UnifiedBenchmarkRunner(sequential_config)
            mock_plugin = MagicMock()
            mock_plugin.name = "sequential_plugin"
            mock_plugin.get_targets.return_value = targets
            sequential_runner.plugin_manager.register_plugin(mock_plugin)
            
            start_time = time.perf_counter()
            sequential_results = sequential_runner.run_all_benchmarks()
            sequential_time = time.perf_counter() - start_time
        
        # ä¸¦åˆ—å®Ÿè¡Œã®æ¸¬å®š
        parallel_config = BenchmarkConfig(
            warmup_runs=1,
            measurement_runs=3,
            parallel=True,
            max_workers=2,
            output_dir=self.config.output_dir
        )
        
        with patch.object(PluginManager, '_auto_discover_plugins'):
            parallel_runner = UnifiedBenchmarkRunner(parallel_config)
            mock_plugin = MagicMock()
            mock_plugin.name = "parallel_plugin"
            mock_plugin.get_targets.return_value = targets
            parallel_runner.plugin_manager.register_plugin(mock_plugin)
            
            start_time = time.perf_counter()
            parallel_results = parallel_runner.run_all_benchmarks()
            parallel_time = time.perf_counter() - start_time
        
        # çµæœæ¤œè¨¼
        self.assertEqual(len(sequential_results["sequential_plugin"]), 4)
        self.assertEqual(len(parallel_results["parallel_plugin"]), 4)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒï¼ˆä¸¦åˆ—å®Ÿè¡Œã®æ–¹ãŒé€Ÿã„ã¯ãšï¼‰
        speedup = sequential_time / parallel_time if parallel_time > 0 else 1
        print(f"ä¸¦åˆ—å®Ÿè¡Œã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—: {speedup:.2f}x (é€æ¬¡: {sequential_time:.3f}s, ä¸¦åˆ—: {parallel_time:.3f}s)")
        
        # ä¸¦åˆ—å®Ÿè¡Œã¯å°‘ãªãã¨ã‚‚åŒç¨‹åº¦ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¤ºã™ã¯ãš
        self.assertLessEqual(parallel_time, sequential_time * 1.2,  # 20%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¾ã§è¨±å®¹
                           f"Parallel execution not efficient: sequential={sequential_time:.3f}s, parallel={parallel_time:.3f}s")
    
    def test_memory_usage_performance(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # å¤§é‡ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å‡¦ç†
        large_results = {}
        for i in range(100):
            large_results[f"memory_target_{i}"] = BenchmarkResult(
                target_name=f"memory_target_{i}",
                plugin_name="memory_test_plugin",
                config={},
                timestamp=time.time(),
                success=True,
                error_message="",
                timing_data=TimingData(
                    warm_up_times=[0.001] * 10,
                    measurement_times=[0.002] * 50,  # å¤§ããªãƒ‡ãƒ¼ã‚¿
                    total_time=0.1,
                    average_time=0.002,
                    std_dev=0.0001,
                    min_time=0.001,
                    max_time=0.003
                ),
                metrics=BenchmarkMetrics(
                    vertices_count=1000,
                    geometry_complexity=5.0,
                    memory_usage=10240,
                    cache_hit_rate=0.9
                ),
                output_data={"large_data": list(range(1000))},  # å¤§ããªãƒ‡ãƒ¼ã‚¿
                serialization_overhead=0.001
            )
        
        # çµæœç®¡ç†ã§ã®å‡¦ç†
        manager = BenchmarkResultManager(str(self.config.output_dir))
        saved_file = manager.save_results(large_results)
        loaded_results = manager.load_results(saved_file)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ : {memory_increase:.1f}MB (åˆæœŸ: {initial_memory:.1f}MB, æœ€çµ‚: {final_memory:.1f}MB)")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒéåº¦ã«å¢—åŠ ã—ã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªï¼ˆ100MBä»¥ä¸‹ï¼‰
        self.assertLess(memory_increase, 100,
                       f"Memory usage increased too much: {memory_increase:.1f}MB")
        
        # çµæœã®æ­£ç¢ºæ€§ã‚‚ç¢ºèª
        self.assertEqual(len(loaded_results), 100)
    
    def test_error_handling_performance(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¤šæ•°ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
        error_targets = []
        for i in range(10):
            target = MagicMock()
            target.name = f"error_target_{i}"
            target.execute.side_effect = RuntimeError(f"Error {i}")
            error_targets.append(target)
        
        executor = BenchmarkExecutor(
            self.config,
            get_error_handler(),
            get_error_collector()
        )
        
        start_time = time.perf_counter()
        
        # è¤‡æ•°ã®ã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†
        for target in error_targets:
            result = executor.initialize_benchmark_result(target)
            try:
                executor.execute_benchmark_measurements(target, result)
            except Exception as e:
                executor.handle_benchmark_exception(result, e)
        
        error_handling_time = time.perf_counter() - start_time
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé«˜é€Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
        self.assertLess(error_handling_time, 0.1,
                       f"Error handling too slow: {error_handling_time:.3f}s for 10 errors")


class TestBenchmarkPerformanceRegression(unittest.TestCase):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    
    def setUp(self):
        self.temp_dir = tempfile.mkdtemp()
        self.baseline_file = Path(self.temp_dir) / "baseline_performance.json"
        
    def tearDown(self):
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_performance_regression_detection(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡ºã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã®ä½œæˆ
        baseline_results = {
            "fast_target": {
                "average_time": 0.001,
                "min_time": 0.0008,
                "max_time": 0.0012
            },
            "slow_target": {
                "average_time": 0.005,
                "min_time": 0.004,
                "max_time": 0.006
            }
        }
        
        # ç¾åœ¨ã®çµæœï¼ˆä¸€éƒ¨ã§æ€§èƒ½åŠ£åŒ–ï¼‰
        current_results = {
            "fast_target": {
                "average_time": 0.0015,  # 50%åŠ£åŒ–
                "min_time": 0.0012,
                "max_time": 0.0018
            },
            "slow_target": {
                "average_time": 0.0045,  # 10%æ”¹å–„
                "min_time": 0.0035,
                "max_time": 0.0055
            }
        }
        
        # å›å¸°æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯
        regressions = []
        improvements = []
        
        for target_name in baseline_results:
            if target_name in current_results:
                baseline_time = baseline_results[target_name]["average_time"]
                current_time = current_results[target_name]["average_time"]
                
                change_ratio = (current_time - baseline_time) / baseline_time
                
                if change_ratio > 0.2:  # 20%ä»¥ä¸Šã®åŠ£åŒ–
                    regressions.append({
                        "target": target_name,
                        "change": change_ratio,
                        "baseline": baseline_time,
                        "current": current_time
                    })
                elif change_ratio < -0.1:  # 10%ä»¥ä¸Šã®æ”¹å–„
                    improvements.append({
                        "target": target_name,
                        "change": change_ratio,
                        "baseline": baseline_time,
                        "current": current_time
                    })
        
        # æ¤œå‡ºçµæœã®æ¤œè¨¼
        self.assertEqual(len(regressions), 1)
        self.assertEqual(regressions[0]["target"], "fast_target")
        self.assertGreater(regressions[0]["change"], 0.4)  # 40%ä»¥ä¸Šã®åŠ£åŒ–
        
        self.assertEqual(len(improvements), 1)
        self.assertEqual(improvements[0]["target"], "slow_target")
        self.assertLess(improvements[0]["change"], -0.05)  # 5%ä»¥ä¸Šã®æ”¹å–„
        
        # å›å¸°ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã®è­¦å‘Š
        if regressions:
            print(f"âš ï¸  Performance regressions detected:")
            for regression in regressions:
                print(f"  {regression['target']}: {regression['change']:+.1%} "
                     f"({regression['baseline']:.3f}s â†’ {regression['current']:.3f}s)")
        
        if improvements:
            print(f"âœ… Performance improvements detected:")
            for improvement in improvements:
                print(f"  {improvement['target']}: {improvement['change']:+.1%} "
                     f"({improvement['baseline']:.3f}s â†’ {improvement['current']:.3f}s)")


if __name__ == '__main__':
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    suite.addTests(loader.loadTestsFromTestCase(TestBenchmarkSystemPerformance))
    
    # å›å¸°æ¤œå‡ºãƒ†ã‚¹ãƒˆ
    suite.addTests(loader.loadTestsFromTestCase(TestBenchmarkPerformanceRegression))
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœã®è¦ç´„
    if result.wasSuccessful():
        print("\nğŸš€ All performance tests passed!")
    else:
        print(f"\nâš ï¸  {len(result.failures)} test(s) failed, {len(result.errors)} error(s) occurred")
        print("Performance regression may have been detected.")